<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MLLM | Winston Hu</title><link>https://WinstonHuTiger.github.io/tag/mllm/</link><atom:link href="https://WinstonHuTiger.github.io/tag/mllm/index.xml" rel="self" type="application/rss+xml"/><description>MLLM</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 18 Jul 2025 21:59:25 -0400</lastBuildDate><image><url>https://WinstonHuTiger.github.io/media/icon_hu60d8226fe61b6d7b6af2c2389494e24b_10520_512x512_fill_lanczos_center_3.png</url><title>MLLM</title><link>https://WinstonHuTiger.github.io/tag/mllm/</link></image><item><title>Efficient Whole Slide Pathology VQA via Token Compression</title><link>https://WinstonHuTiger.github.io/project/effecient_mllm_token_compression/</link><pubDate>Fri, 18 Jul 2025 21:59:25 -0400</pubDate><guid>https://WinstonHuTiger.github.io/project/effecient_mllm_token_compression/</guid><description>&lt;p>Whole-slide images (WSIs) in pathology can reach up
to 100,000 Ã—100,000 pixels, posing significant challenges
for multimodal large language model (MLLM) due to long
context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-
level classification using CLIP-based models with multi-
instance learning, but they lack the generative capabilities
needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thou-
sands of patch tokens directly into the language model,
which leads to excessive resource consumption. To address
these limitations, we propose &lt;strong>Token Compression Pathol-
ogy LLaVA (TCP-LLaVA)&lt;/strong>, the first MLLM architecture to
perform WSI VQA via token compression. TCP-LLaVA in-
troduces a set of trainable compression tokens that aggre-
gate visual and textual information through a modality com-
pression module, inspired by the [CLS] token mechanism
in BERT.&lt;/p></description></item></channel></rss>
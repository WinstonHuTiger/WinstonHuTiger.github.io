<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computational Pathology | Winston Hu</title><link>https://WinstonHuTiger.github.io/tag/computational-pathology/</link><atom:link href="https://WinstonHuTiger.github.io/tag/computational-pathology/index.xml" rel="self" type="application/rss+xml"/><description>Computational Pathology</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 18 Jul 2025 21:59:25 -0400</lastBuildDate><image><url>https://WinstonHuTiger.github.io/media/icon_hu60d8226fe61b6d7b6af2c2389494e24b_10520_512x512_fill_lanczos_center_3.png</url><title>Computational Pathology</title><link>https://WinstonHuTiger.github.io/tag/computational-pathology/</link></image><item><title>Efficient Whole Slide Pathology VQA via Token Compression</title><link>https://WinstonHuTiger.github.io/project/effecient_mllm_token_compression/</link><pubDate>Fri, 18 Jul 2025 21:59:25 -0400</pubDate><guid>https://WinstonHuTiger.github.io/project/effecient_mllm_token_compression/</guid><description>&lt;DIV align="justify">
In this project, we propose the very first MLLM architecture, named &lt;b>Token Compression Pathology LLaVA (TCP-LLaVA)&lt;/b> to perform WSI VQA via token compression.
&lt;/DiV>
&lt;h2 id="abstract">&lt;strong>Abstract&lt;/strong>&lt;/h2>
&lt;DIV align="justify">
Whole-slide images (WSIs) in pathology can reach up
to 100,000 Ã—100,000 pixels, posing significant challenges
for multimodal large language model (MLLM) due to long
context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification using CLIP-based models with multi-
instance learning, but they lack the generative capabilities
needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly into the language model,
which leads to excessive resource consumption. To address
these limitations, we propose &lt;b>Token Compression Pathology LLaVA (TCP-LLaVA)&lt;/b>, the first MLLM architecture to
perform WSI VQA via token compression. TCP-LLaVA introduces a set of trainable compression tokens that aggregate visual and textual information through a modality compression module, inspired by the [CLS] token mechanism in BERT.
&lt;/DIV></description></item></channel></rss>
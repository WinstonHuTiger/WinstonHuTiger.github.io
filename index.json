[{"authors":null,"categories":null,"content":"Hi, I am Qingqiao (Winston) Hu, a CS PhD student at Stony Brook University. I am doing research in machine learning and medical imaging analysis with Prof. Chao Chen. I finished my master at UCLA ECE department, working with Prof. Vwani Roychowdhury. I work closely with Prof. Jianguo Zhang and Dr. Hongwei (Bran) Li on medical imaging analysis during my master and my bachelor.\nDownload my resumé .\n","date":1724821473,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1724821473,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi, I am Qingqiao (Winston) Hu, a CS PhD student at Stony Brook University. I am doing research in machine learning and medical imaging analysis with Prof. Chao Chen. I finished my master at UCLA ECE department, working with Prof.","tags":null,"title":"Winston Hu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://WinstonHuTiger.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Weimin Lyu","Qingqiao Hu","Kehan Qi","Zhan Shi","Wentao Huang","Saumya Gupta","Chao Chen"],"categories":[],"content":"Whole-slide images (WSIs) in pathology can reach up to 100,000 ×100,000 pixels, posing significant challenges for multimodal large language model (MLLM) due to long context length and high computational demands. Previous methods typically focus on patch-level analysis or slide- level classification using CLIP-based models with multi- instance learning, but they lack the generative capabilities needed for visual question answering (VQA). More recent MLLM-based approaches address VQA by feeding thou- sands of patch tokens directly into the language model, which leads to excessive resource consumption. To address these limitations, we propose Token Compression Pathol- ogy LLaVA (TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token compression. TCP-LLaVA in- troduces a set of trainable compression tokens that aggre- gate visual and textual information through a modality com- pression module, inspired by the [CLS] token mechanism in BERT.\n","date":1752890365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752890365,"objectID":"88e0e770c82e99aa93dc7610bdcbc00a","permalink":"https://WinstonHuTiger.github.io/project/effecient_mllm_token_compression/","publishdate":"2025-07-18T21:59:25-04:00","relpermalink":"/project/effecient_mllm_token_compression/","section":"project","summary":"Whole-slide images (WSIs) in pathology can reach up to 100,000 ×100,000 pixels, posing significant challenges for multimodal large language model (MLLM) due to long context length and high computational demands. Previous methods typically focus on patch-level analysis or slide- level classification using CLIP-based models with multi- instance learning, but they lack the generative capabilities needed for visual question answering (VQA).","tags":["Computational Pathology","MLLM","Token Compression"],"title":"Efficient Whole Slide Pathology VQA via Token Compression","type":"project"},{"authors":["Xiaotong Zhang","Qingqiao Hu","Zhen Xiao","Tao Sun","Jiaxi Zhang","Jin Zhang","Zhenjiang Li"],"categories":[],"content":"","date":1726700717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726700717,"objectID":"44bc63c9c3784218d0623c8057213d5b","permalink":"https://WinstonHuTiger.github.io/publication/my_tmc_few_shot_2024/","publishdate":"2024-09-18T19:05:17-04:00","relpermalink":"/publication/my_tmc_few_shot_2024/","section":"publication","summary":"Wireless-based human activity recognition (WHAR) enables various promising applications. However, since WHAR is sensitive to changes in sensing conditions (e.g., different environments, users, and new activities), trained models often do not work well under new conditions. Recent research uses meta-learning to adapt models. However, they must fine-tune the model, which greatly hinders the widespread adoption of WHAR in practice because model fine-tuning is difficult to automate and requires deep-learning expertise. The fundamental reason for model fine-tuning in existing works is because their goal is to find the mapping relationship between data samples and corresponding activity labels. Since this mapping reflects the intrinsic properties of data in the perceptual scene, it is naturally related to the conditions under which the activity is sensed. To address this problem, we exploit the principle that under the same sensing condition, data of the same activity class are more similar (in a certain latent space) than data of other classes, and this property holds invariant across different conditions. Our main observation is that meta-learning can actually also transform WHAR design into a learning problem that is always under similar conditions, thus decoupling the dependence on sensing conditions. With this capability, general and accurate WHAR can be achieved, avoiding model fine-tuning. In this paper, we implement this idea through two innovative designs in a system called RoMF. Extensive experiments using FMCW, Wi-Fi and acoustic three sensing signals show that it can achieve up to 95.3% accuracy in unseen conditions, including new environments, users and activity classes.","tags":["Few shot learning","Wireless-based human activity recognition","Deep Learning"],"title":"Few-Shot Adaptation to Unseen Conditions for Wireless-Based Human Activity Recognition Without Fine-Tuning","type":"publication"},{"authors":["Qingqiao Hu","Daoan Zhang","Jiebo Luo","Zhenyu Gong","Benedikt Wiestler","Jianguo Zhang","Hongwei Bran Li"],"categories":[],"content":"In this project, we investigate the State Space Models, especially Mamba. We use Masked Image Modeling pre-training strategy, and fintune the model for 3D radiomics analysis. We evaluate the model in terms of downstream classification task and its latent space interpretability.\n","date":1726192765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726192765,"objectID":"eb8f545adf78ec9a4e216d59d3a8430c","permalink":"https://WinstonHuTiger.github.io/project/3d_mamba_mae/","publishdate":"2024-09-12T21:59:25-04:00","relpermalink":"/project/3d_mamba_mae/","section":"project","summary":"In this project, we investigate the State Space Models, especially Mamba. We use Masked Image Modeling pre-training strategy, and fintune the model for 3D radiomics analysis. We evaluate the model in terms of downstream classification task and its latent space interpretability.","tags":["State space models","Radiomics analysis","representation learning"],"title":"Learning Brain Tumor Representation in 3D High-Resolution MR Images via Interpretable State Space Models","type":"project"},{"authors":["Winston Hu"],"categories":[],"content":"","date":1724821473,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724821473,"objectID":"17a1875b5c96f30f0653fe063aa50ac4","permalink":"https://WinstonHuTiger.github.io/post/starting_phd/","publishdate":"2024-08-28T01:04:33-04:00","relpermalink":"/post/starting_phd/","section":"post","summary":"Start my PhD journey at Stony Brook University","tags":[],"title":"Start my PhD journey at Stony Brook University","type":"post"},{"authors":["Qingqiao Hu","Hao Wang","Jing Luo","Yunhao Luo","Zhiheng Zhangg","Jan S. Kirschke","Benedikt Wiestler","Bjoern Menze","Jianguo Zhang","Hongwei Bran Li"],"categories":[],"content":"","date":1688269594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688269594,"objectID":"e6c5464c6a3769223acf0f276dc18899","permalink":"https://WinstonHuTiger.github.io/publication/my_qubiq_uncertainty/","publishdate":"2023-07-01T20:46:34-07:00","relpermalink":"/publication/my_qubiq_uncertainty/","section":"publication","summary":"Automated medical image segmentation inherently involves a certain degree of uncertainty. One key factor contributing to this uncertainty is the ambiguity that can arise in determining the boundaries of a target region of interest, primarily due to variations in image appearance. On top of this, even among experts in the field, different opinions can emerge regarding the precise definition of specific anatomical structures. This work specifically addresses the modeling of segmentation uncertainty, known as inter-rater uncertainty. Its primary objective is to explore and analyze the variability in segmentation outcomes that can occur when multiple experts in medical imaging interpret and annotate the same images. We introduce a novel Bayesian neural network-based architecture to estimate inter-rater uncertainty in medical image segmentation. Our approach has three key advancements. Firstly, we introduce a one-encoder-multi-decoder architecture specifically tailored for uncertainty estimation, enabling us to capture the rater-specific representation of each expert involved. Secondly, we propose Bayesian modeling for the new architecture, allowing efficient capture of the inter-rater distribution, particularly in scenarios with limited annotations. Lastly, we enhance the rater-specific representation by integrating an attention module into each decoder. This module facilitates focused and refined segmentation results for each rater. We also provide an open-source 3D multi-rater dataset with three raters annotated liver lesions in CT images as an additional contribution. This dataset serves as an independent resource for further research in the field. We conduct extensive evaluations using synthetic and real-world datasets to validate our technical innovations rigorously. Our method surpasses existing baseline methods in five out of seven diverse tasks on the publicly available \u001bmph{QUBIQ} dataset, considering two evaluation metrics encompassing different uncertainty aspects. Furthermore, on the \u001bmph{LIDC-IDRI} dataset, our approach demonstrates superior performance and accurately captures the inter-rater distribution of segmentations. Our codes, models, and the new dataset are available through our GitHub repository: https://github.com/HaoWang420/bOEMD-net","tags":["Uncertainty quantification","Segmentation","Inter-rater variability","Deep Learning"],"title":"Inter-Rater Uncertainty Quantification in Medical Image Segmentation via Rater-Specific Bayesian Neural Networks","type":"publication"},{"authors":["Qingqiao Hu","Hao Wang","Jing Luo","Yunhao Luo","Zhiheng Zhang","Jan S. Kirschke","Benedikt Wiestler","Bjoern Menze","Jianguo Zhang","Hongwei Bran Li"],"categories":[],"content":" In this project, we1 proposed a simple yet effective Bayesian Neural Network (BNN) framework, especifically designed for inter-rater uncertainty quantification probelm in medical imaging. We call this framework, Bayesian One-Encoder-Multiple-Decoder (BOEMD) network.\nA qualitative comparison was performed among the probabilistic U-Net, PHiSeg and BOEMD. The error map quantifies the difference between predicted and observed distributions. From the segmentation outputs and the error map, one can see that our method captures inter-rater uncertainty more accurately compared to the others. Automated medical image segmentation inherently involves a certain degree of uncerainty. One key factor contributing to this uncertainty is the ambiguity that can arise in determining the boundaries of a target region of interest, primarily due to variations in image appearance. On top of this, even among experts in the field, different opinions can emerge regarding the precise definition of specific anatomical structures. Such modeling of segmentation uncertainty, known as inter-rater uncertainty is explored and analyzed in this project.\nA schematic view of our architecture to estimate inter-rater uncertainty with Bayesian modeling. It contains one Bayesian encoder and multiple Bayesian decoders. In the skip connection (red, blue, and green) between the encoder and each decoder, an attention module (denoted by A) is introduced to capture rater-specific representation for individual raters. BOEMD has three main features:\nOne-Encoder-Multiple-Decoder: the only one encoder is designed to learn the shared knowledge from segmentation while the multiple decoders that are aligned with each rater (if the rater ids are known, the encoder tends to learn the specific features from each rater; otherwise, the rater ids are unknown, each decoder learns the shared knowledge as the encoder does.) Rater-specific Attention: each attention unit is associated with one decoder, to further enhance the ability of decoder capturing the specific knowledge from segmentation map. Bayesian Modelling: usually medical imaging dataset is quite small compared to other existing vision dataset, which results in the neural network over-fitting. Adopting Bayesian Modelling in our framework helps the neural network learn the dataset distribution from a high level. Doing this also reduces over-fitting. We also introduced a rater-aligned liver tumor dataset in this work to further demonstrate the intention of our design.\nQingqiao Hu and Hao Wang made equal contribution to this project. ↩︎\n","date":1687307432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687307432,"objectID":"54b83032c201051af91db4ec6b2d7c17","permalink":"https://WinstonHuTiger.github.io/project/qubiq_uncertainty_quanification/","publishdate":"2023-06-20T17:30:32-07:00","relpermalink":"/project/qubiq_uncertainty_quanification/","section":"project","summary":"In this project, we1 proposed a simple yet effective Bayesian Neural Network (BNN) framework, especifically designed for inter-rater uncertainty quantification probelm in medical imaging. We call this framework, Bayesian One-Encoder-Multiple-Decoder (BOEMD) network.","tags":["Uncertainty quantification","Segmentation","Inter-rater variability","Deep Learning"],"title":"Bayeisan One-Encoder-Multiple-Decoder (BOEMD) Network","type":"project"},{"authors":["Qingqiao Hu","Hongwei Li","Jianguo Zhang"],"categories":[],"content":"","date":1663372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663372800,"objectID":"271f30b5a84940618fcc1083957bfdf5","permalink":"https://WinstonHuTiger.github.io/publication/my_miccai_2022_domain_adaptation/","publishdate":"2022-11-19T10:52:37-08:00","relpermalink":"/publication/my_miccai_2022_domain_adaptation/","section":"publication","summary":"A novel efficient domain adaptation approach based on 2D VAE for 3D medical synthesis task.","tags":[],"title":"Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://WinstonHuTiger.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://WinstonHuTiger.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]